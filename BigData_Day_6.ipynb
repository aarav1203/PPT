{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syPcfFS2HDBe",
        "outputId": "78b7b6d6-fa80-41d3-8281-a1e6c8109ef4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=59d4a23d96bd198189a6b6bdd68e05fabfbc57e855675b8678f9bf566ef2bdb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Working with RDDs:\n",
        "   a) Write a Python program to create an RDD from a local data source.\n"
      ],
      "metadata": {
        "id": "MsafgZu7HtOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Example\")\n",
        "\n",
        "# Create an RDD from a local data source\n",
        "rdd = sc.textFile(\"path/to/local/file.txt\")\n",
        "\n",
        "# Perform operations on the RDD\n",
        "# For example, let's count the number of lines in the file\n",
        "line_count = rdd.count()\n",
        "\n",
        "# Print the line count\n",
        "print(\"Number of lines in the file:\", line_count)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Et7SqxZ8HkOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   b) Implement transformations and actions on the RDD to perform data processing tasks."
      ],
      "metadata": {
        "id": "MWypV2KkHosN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Example\")\n",
        "\n",
        "# Create an RDD from a local data source\n",
        "rdd = sc.textFile(\"path/to/local/file.txt\")\n",
        "\n",
        "# Perform transformations and actions on the RDD\n",
        "\n",
        "# 1. Filter transformation: Keep lines containing the word \"error\"\n",
        "error_lines = rdd.filter(lambda line: \"error\" in line.lower())\n",
        "\n",
        "# 2. Map transformation: Extract words from each line\n",
        "words = rdd.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# 3. ReduceByKey transformation: Count the occurrence of each word\n",
        "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# 4. Take action: Print the first 10 word counts\n",
        "top_word_counts = word_counts.take(10)\n",
        "for word, count in top_word_counts:\n",
        "    print(word, count)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BJG0k44rII4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
        "\n"
      ],
      "metadata": {
        "id": "UO5z6kLFIPhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Example\")\n",
        "\n",
        "# Create an RDD from a local data source\n",
        "rdd = sc.textFile(\"path/to/local/file.txt\")\n",
        "\n",
        "# Perform data analysis and manipulation using RDD operations\n",
        "\n",
        "# Map operation: Extract the length of each line\n",
        "line_lengths = rdd.map(lambda line: len(line))\n",
        "\n",
        "# Filter operation: Keep lines with length greater than 50\n",
        "long_lines = rdd.filter(lambda line: len(line) > 50)\n",
        "\n",
        "# Reduce operation: Calculate the total length of all lines\n",
        "total_length = line_lengths.reduce(lambda a, b: a + b)\n",
        "\n",
        "# Aggregate operation: Calculate the average length of lines\n",
        "total_count = rdd.count()\n",
        "average_length = total_length / total_count\n",
        "\n",
        "# Print the analysis results\n",
        "print(\"Average line length:\", average_length)\n",
        "print(\"Long lines:\")\n",
        "for line in long_lines.collect():\n",
        "    print(line)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "s0zGwjz0IZPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Spark DataFrame Operations:\n",
        "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n"
      ],
      "metadata": {
        "id": "q-wRGxMCIeFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
        "\n",
        "# Load a CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/csv/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the contents of the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fY4mTblVIoJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   b)Perform common DataFrame operations such as filtering, grouping, or joining."
      ],
      "metadata": {
        "id": "FyH-ZC1-Isw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrame Operations\").getOrCreate()\n",
        "\n",
        "# Load a CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/csv/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Perform DataFrame operations\n",
        "\n",
        "# Filtering: Keep rows where the age is greater than 30\n",
        "filtered_df = df.filter(df.age > 30)\n",
        "\n",
        "# Grouping: Calculate the average salary for each department\n",
        "grouped_df = df.groupBy(\"department\").avg(\"salary\")\n",
        "\n",
        "# Joining: Perform an inner join with another DataFrame\n",
        "other_df = spark.read.csv(\"path/to/other/csv/file.csv\", header=True, inferSchema=True)\n",
        "joined_df = df.join(other_df, df.id == other_df.id, \"inner\")\n",
        "\n",
        "# Show the results\n",
        "print(\"Filtered DataFrame:\")\n",
        "filtered_df.show()\n",
        "\n",
        "print(\"Grouped DataFrame:\")\n",
        "grouped_df.show()\n",
        "\n",
        "print(\"Joined DataFrame:\")\n",
        "joined_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cM0Ga3N4I1fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data."
      ],
      "metadata": {
        "id": "wLk5SwVgI5Tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Example\").getOrCreate()\n",
        "\n",
        "# Load a CSV file into a DataFrame\n",
        "df = spark.read.csv(\"path/to/csv/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Register the DataFrame as a temporary table\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# Apply Spark SQL queries on the DataFrame\n",
        "\n",
        "# Example 1: Retrieve all records\n",
        "query1 = \"SELECT * FROM employees\"\n",
        "result1 = spark.sql(query1)\n",
        "\n",
        "# Example 2: Calculate the average salary\n",
        "query2 = \"SELECT AVG(salary) AS avg_salary FROM employees\"\n",
        "result2 = spark.sql(query2)\n",
        "\n",
        "# Example 3: Filter and sort the records\n",
        "query3 = \"SELECT name, age, department FROM employees WHERE age > 30 ORDER BY age DESC\"\n",
        "result3 = spark.sql(query3)\n",
        "\n",
        "# Show the results\n",
        "print(\"Query 1 - All records:\")\n",
        "result1.show()\n",
        "\n",
        "print(\"Query 2 - Average salary:\")\n",
        "result2.show()\n",
        "\n",
        "print(\"Query 3 - Filtered and sorted records:\")\n",
        "result3.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "VnSRT8V1JFjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Spark Streaming:\n",
        "  a) Write a Python program to create a Spark Streaming application.\n"
      ],
      "metadata": {
        "id": "cEAXpPLoJTmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local[2]\", \"Spark Streaming Example\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "# Create a DStream from a TCP socket as a streaming source\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Perform operations on the DStream\n",
        "\n",
        "# Example: Word count\n",
        "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "                  .map(lambda word: (word, 1)) \\\n",
        "                  .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming computation\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the streaming computation to finish\n",
        "ssc.awaitTermination()\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "yal5h0eXJWyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket)."
      ],
      "metadata": {
        "id": "m8qIgJFvJazF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local[2]\", \"Spark Streaming Example\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "# Configure the application to consume data from a streaming source\n",
        "\n",
        "# Option 1: Consume data from a Kafka topic\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "kafka_params = {\n",
        "    \"bootstrap.servers\": \"localhost:9092\",\n",
        "    \"group.id\": \"spark-streaming-consumer-group\"\n",
        "}\n",
        "\n",
        "topics = [\"my-topic\"]\n",
        "kafka_stream = KafkaUtils.createDirectStream(ssc, topics, kafka_params)\n",
        "\n",
        "# Option 2: Consume data from a socket\n",
        "socket_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Perform operations on the DStream\n",
        "\n",
        "# Example: Word count\n",
        "word_counts = kafka_stream.flatMap(lambda line: line[1].split(\" \")) \\\n",
        "                          .map(lambda word: (word, 1)) \\\n",
        "                          .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the streaming computation\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the streaming computation to finish\n",
        "ssc.awaitTermination()\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9k5QlIyUJlAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   c) Implement streaming transformations and actions to process and analyze the incoming data stream."
      ],
      "metadata": {
        "id": "b4KYaH4lJpfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a SparkContext\n",
        "sc = SparkContext(\"local[2]\", \"Spark Streaming Example\")\n",
        "\n",
        "# Create a StreamingContext with a batch interval of 1 second\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "# Configure the application to consume data from a streaming source\n",
        "socket_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "# Implement streaming transformations and actions\n",
        "\n",
        "# Example: Word count\n",
        "word_counts = socket_stream.flatMap(lambda line: line.split(\" \")) \\\n",
        "                          .map(lambda word: (word, 1)) \\\n",
        "                          .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts\n",
        "word_counts.pprint()\n",
        "\n",
        "# Example: Compute average length of lines\n",
        "line_lengths = socket_stream.map(lambda line: len(line))\n",
        "total_length = line_lengths.reduce(lambda a, b: a + b)\n",
        "line_count = line_lengths.count()\n",
        "average_length = total_length / line_count\n",
        "\n",
        "# Print the average line length\n",
        "print(\"Average line length:\", average_length)\n",
        "\n",
        "# Start the streaming computation\n",
        "ssc.start()\n",
        "\n",
        "# Wait for the streaming computation to finish\n",
        "ssc.awaitTermination()\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9sN8MgtiJwxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Spark SQL and Data Source Integration:\n",
        "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n"
      ],
      "metadata": {
        "id": "tpXbvyAFJ16d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Example\").getOrCreate()\n",
        "\n",
        "# Configure the connection to the relational database\n",
        "\n",
        "# MySQL example:\n",
        "url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
        "properties = {\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\",\n",
        "    \"user\": \"username\",\n",
        "    \"password\": \"password\"\n",
        "}\n",
        "\n",
        "# PostgreSQL example:\n",
        "# url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
        "# properties = {\n",
        "#     \"driver\": \"org.postgresql.Driver\",\n",
        "#     \"user\": \"username\",\n",
        "#     \"password\": \"password\"\n",
        "# }\n",
        "\n",
        "# Load data from a table in the relational database\n",
        "table_name = \"my_table\"\n",
        "df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
        "\n",
        "# Perform operations on the DataFrame\n",
        "# Example: Show the contents of the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2Aed4F2GJ_tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   b)Perform SQL operations on the data stored in the database using Spark SQL."
      ],
      "metadata": {
        "id": "_ifVYg5aKEvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark SQL Example\").getOrCreate()\n",
        "\n",
        "# Configure the connection to the relational database\n",
        "\n",
        "# MySQL example:\n",
        "url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
        "properties = {\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\",\n",
        "    \"user\": \"username\",\n",
        "    \"password\": \"password\"\n",
        "}\n",
        "\n",
        "# PostgreSQL example:\n",
        "# url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
        "# properties = {\n",
        "#     \"driver\": \"org.postgresql.Driver\",\n",
        "#     \"user\": \"username\",\n",
        "#     \"password\": \"password\"\n",
        "# }\n",
        "\n",
        "# Register the table as a temporary view\n",
        "table_name = \"my_table\"\n",
        "df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
        "df.createOrReplaceTempView(\"my_table_view\")\n",
        "\n",
        "# Perform SQL operations on the data stored in the database\n",
        "\n",
        "# Example 1: Retrieve all records\n",
        "query1 = \"SELECT * FROM my_table_view\"\n",
        "result1 = spark.sql(query1)\n",
        "\n",
        "# Example 2: Calculate the average value\n",
        "query2 = \"SELECT AVG(value) AS avg_value FROM my_table_view\"\n",
        "result2 = spark.sql(query2)\n",
        "\n",
        "# Example 3: Filter the records\n",
        "query3 = \"SELECT * FROM my_table_view WHERE category = 'A'\"\n",
        "result3 = spark.sql(query3)\n",
        "\n",
        "# Show the results\n",
        "print(\"Query 1 - All records:\")\n",
        "result1.show()\n",
        "\n",
        "print(\"Query 2 - Average value:\")\n",
        "result2.show()\n",
        "\n",
        "print(\"Query 3 - Filtered records:\")\n",
        "result3.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PDZDPa1RKMsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3."
      ],
      "metadata": {
        "id": "vcTH53cMKSu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark S3 Example\").getOrCreate()\n",
        "\n",
        "# Configure AWS credentials\n",
        "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", \"your_access_key\")\n",
        "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"your_secret_key\")\n",
        "\n",
        "# Read data from Amazon S3\n",
        "s3_path = \"s3a://bucket-name/path/to/data\"\n",
        "df = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
        "\n",
        "# Perform operations on the DataFrame\n",
        "# ...\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OGdgw3jQKdQz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7E0zF2hjHlqE"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}