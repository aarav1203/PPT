{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n"
      ],
      "metadata": {
        "id": "gRUDcW5GDIRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import configparser\n",
        "\n",
        "def display_hadoop_components(config_file):\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(config_file)\n",
        "\n",
        "    if 'core-site' in config:\n",
        "        print('Core components:')\n",
        "        core_components = config['core-site'].get('fs.defaultFS')\n",
        "        print(core_components)\n",
        "    else:\n",
        "        print('No core components found in the configuration file.')\n",
        "\n",
        "    if 'hdfs-site' in config:\n",
        "        print('\\nHDFS components:')\n",
        "        namenode = config['hdfs-site'].get('dfs.namenode.rpc-address')\n",
        "        datanodes = config['hdfs-site'].get('dfs.datanode.data.dir')\n",
        "        print(f'Namenode: {namenode}')\n",
        "        print(f'Datanodes: {datanodes}')\n",
        "    else:\n",
        "        print('No HDFS components found in the configuration file.')\n",
        "\n",
        "# Specify the path to your Hadoop configuration file\n",
        "config_file = '/path/to/hadoop/config/file.xml'\n",
        "\n",
        "# Call the function to display the components\n",
        "display_hadoop_components(config_file)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "chJoA0krDR36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory"
      ],
      "metadata": {
        "id": "fkaiBTE1DYSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import pyarrow.hdfs as hdfs\n",
        "\n",
        "def calculate_directory_size(hdfs_host, hdfs_port, hdfs_directory):\n",
        "    fs = hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
        "    total_size = 0\n",
        "\n",
        "    # Recursively iterate over the files in the directory\n",
        "    for path, _, files in fs.walk(hdfs_directory):\n",
        "        for file_name in files:\n",
        "            file_path = f\"{path}/{file_name}\"\n",
        "            file_info = fs.info(file_path)\n",
        "            file_size = file_info.size\n",
        "            total_size += file_size\n",
        "\n",
        "    fs.close()\n",
        "\n",
        "    return total_size\n",
        "\n",
        "# Set the HDFS host, port, and directory path\n",
        "hdfs_host = 'localhost'\n",
        "hdfs_port = 9000\n",
        "hdfs_directory = '/path/to/hdfs/directory'\n",
        "\n",
        "# Call the function to calculate the total file size\n",
        "total_file_size = calculate_directory_size(hdfs_host, hdfs_port, hdfs_directory)\n",
        "\n",
        "print(f\"Total file size in HDFS directory '{hdfs_directory}': {total_file_size} bytes\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7S5b1_1VDrLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
      ],
      "metadata": {
        "id": "guxLjZzbD7R3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import re\n",
        "\n",
        "class TopNWords(MRJob):\n",
        "\n",
        "    def configure_args(self):\n",
        "        super(TopNWords, self).configure_args()\n",
        "        self.add_passthru_arg('--N', type=int, default=10, help='Number of top words to display')\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper,\n",
        "                   combiner=self.combiner,\n",
        "                   reducer=self.reducer),\n",
        "            MRStep(reducer=self.final_reducer)\n",
        "        ]\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        # Split the line into words\n",
        "        words = re.findall(r'\\w+', line.lower())\n",
        "\n",
        "        # Emit each word with a count of 1\n",
        "        for word in words:\n",
        "            yield word, 1\n",
        "\n",
        "    def combiner(self, word, counts):\n",
        "        # Sum the counts of the words\n",
        "        yield word, sum(counts)\n",
        "\n",
        "    def reducer(self, word, counts):\n",
        "        # Sum the counts of the words\n",
        "        yield None, (sum(counts), word)\n",
        "\n",
        "    def final_reducer(self, _, word_counts):\n",
        "        N = self.options.N\n",
        "        # Sort the word counts in descending order\n",
        "        sorted_word_counts = sorted(word_counts, reverse=True)\n",
        "\n",
        "        # Extract the top N most frequent words\n",
        "        top_words = sorted_word_counts[:N]\n",
        "\n",
        "        # Yield each top word with its count\n",
        "        for count, word in top_words:\n",
        "            yield word, count\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper,\n",
        "                   combiner=self.combiner,\n",
        "                   reducer=self.reducer),\n",
        "            MRStep(reducer=self.final_reducer)\n",
        "        ]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TopNWords.run()\n",
        "```\n",
        "To run the program, save it to a file (e.g., top_n_words.py) and execute it with the following command, specifying the input file and the value of N for the number of top words to display:\n",
        "\n",
        "\n",
        "```\n",
        "python top_n_words.py 5\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J7zmzgDTEFQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
      ],
      "metadata": {
        "id": "kH9D-Sd8FDiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import requests\n",
        "\n",
        "def check_namenode_health(nn_host, nn_port):\n",
        "    url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        live_nodes = data['beans'][0]['LiveNodes']\n",
        "        return live_nodes['NumLiveDataNodes']\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def check_datanode_health(nn_host, nn_port):\n",
        "    url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        live_nodes = data['beans'][0]['LiveNodes']\n",
        "        return len(live_nodes)\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Set the NameNode host and port\n",
        "nn_host = 'localhost'\n",
        "nn_port = 50070\n",
        "\n",
        "# Check NameNode health status\n",
        "nn_health = check_namenode_health(nn_host, nn_port)\n",
        "if nn_health != -1:\n",
        "    print(f\"NameNode is healthy. Number of live DataNodes: {nn_health}\")\n",
        "else:\n",
        "    print(\"Failed to retrieve NameNode health status.\")\n",
        "\n",
        "# Check DataNode health status\n",
        "dn_health = check_datanode_health(nn_host, nn_port)\n",
        "if dn_health != -1:\n",
        "    print(f\"Number of live DataNodes: {dn_health}\")\n",
        "else:\n",
        "    print(\"Failed to retrieve DataNode health status.\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "HRY_4VbzFMII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n"
      ],
      "metadata": {
        "id": "ZSLjc-IHFRvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import pyarrow.hdfs as hdfs\n",
        "\n",
        "def list_hdfs_path(hdfs_host, hdfs_port, hdfs_path):\n",
        "    fs = hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
        "\n",
        "    # List all files and directories in the HDFS path\n",
        "    file_status_list = fs.ls(hdfs_path)\n",
        "\n",
        "    # Print the files and directories\n",
        "    print(f\"Contents of {hdfs_path}:\")\n",
        "    for file_status in file_status_list:\n",
        "        print(file_status['name'])\n",
        "\n",
        "    fs.close()\n",
        "\n",
        "# Set the HDFS host, port, and path\n",
        "hdfs_host = 'localhost'\n",
        "hdfs_port = 9000\n",
        "hdfs_path = '/path/to/hdfs/directory'\n",
        "\n",
        "# Call the function to list the files and directories\n",
        "list_hdfs_path(hdfs_host, hdfs_port, hdfs_path)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1d8z561GFXnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n"
      ],
      "metadata": {
        "id": "qFc7-9MCFcqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import requests\n",
        "\n",
        "def analyze_data_nodes_storage(nn_host, nn_port):\n",
        "    url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        live_nodes = data['beans'][0]['LiveNodes']\n",
        "        \n",
        "        # Retrieve storage utilization information for each DataNode\n",
        "        node_usages = {}\n",
        "        for node_id, node_info in live_nodes.items():\n",
        "            node_name = node_info['name']\n",
        "            storage_capacity = node_info['capacity']\n",
        "            storage_used = node_info['used']\n",
        "            storage_remaining = node_info['remaining']\n",
        "            storage_utilization = (storage_used / storage_capacity) * 100\n",
        "            node_usages[node_name] = storage_utilization\n",
        "        \n",
        "        # Find node with the highest storage utilization\n",
        "        highest_utilization_node = max(node_usages, key=node_usages.get)\n",
        "        \n",
        "        # Find node with the lowest storage utilization\n",
        "        lowest_utilization_node = min(node_usages, key=node_usages.get)\n",
        "        \n",
        "        return node_usages, highest_utilization_node, lowest_utilization_node\n",
        "    else:\n",
        "        return {}, None, None\n",
        "\n",
        "# Set the NameNode host and port\n",
        "nn_host = 'localhost'\n",
        "nn_port = 50070\n",
        "\n",
        "# Analyze data nodes storage utilization\n",
        "node_usages, highest_utilization_node, lowest_utilization_node = analyze_data_nodes_storage(nn_host, nn_port)\n",
        "\n",
        "if node_usages:\n",
        "    print(\"Data Node Storage Utilization:\")\n",
        "    for node, utilization in node_usages.items():\n",
        "        print(f\"{node}: {utilization:.2f}%\")\n",
        "\n",
        "    print(f\"\\nNode with the highest storage utilization: {highest_utilization_node} ({node_usages[highest_utilization_node]:.2f}%)\")\n",
        "    print(f\"Node with the lowest storage utilization: {lowest_utilization_node} ({node_usages[lowest_utilization_node]:.2f}%)\")\n",
        "else:\n",
        "    print(\"Failed to retrieve data node storage utilization.\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XtyTk4YOFlxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
      ],
      "metadata": {
        "id": "PGzO8xXhFqGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_name, jar_path, main_class, input_path, output_path):\n",
        "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    # Prepare the job submission payload\n",
        "    payload = {\n",
        "        \"application\": {\n",
        "            \"application-name\": job_name,\n",
        "            \"am-container-spec\": {\n",
        "                \"commands\": {\n",
        "                    \"command\": f\"yarn jar {jar_path} {main_class} {input_path} {output_path}\"\n",
        "                }\n",
        "            },\n",
        "            \"max-attempts\": 1,\n",
        "            \"resource\": {\n",
        "                \"memory\": 1024,\n",
        "                \"vCores\": 1\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Submit the job\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    if response.status_code == 202:\n",
        "        data = response.json()\n",
        "        app_id = data['app']['id']\n",
        "        print(f\"Job submitted successfully. Application ID: {app_id}\")\n",
        "        return app_id\n",
        "    else:\n",
        "        print(\"Failed to submit the job.\")\n",
        "        return None\n",
        "\n",
        "def monitor_job_progress(resourcemanager_host, resourcemanager_port, app_id):\n",
        "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{app_id}\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            app_status = data['app']['state']\n",
        "            if app_status in ('FINISHED', 'FAILED', 'KILLED'):\n",
        "                print(f\"Job status: {app_status}\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Job status: {app_status}\")\n",
        "        else:\n",
        "            print(\"Failed to get job status.\")\n",
        "            break\n",
        "\n",
        "        time.sleep(5)\n",
        "\n",
        "def retrieve_job_output(resourcemanager_host, resourcemanager_port, app_id, output_path):\n",
        "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{app_id}/appattempts\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        attempt_id = data['appAttempts']['appAttempt'][0]['appAttemptId']\n",
        "        container_id = data['appAttempts']['appAttempt'][0]['containerId']\n",
        "        logs_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/proxy/{app_id}/node/{container_id}/logFiles/syslog/?start=-4096\"\n",
        "\n",
        "        response = requests.get(logs_url)\n",
        "        if response.status_code == 200:\n",
        "            with open(output_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Job output retrieved and saved to: {output_path}\")\n",
        "        else:\n",
        "            print(\"Failed to retrieve job output.\")\n",
        "    else:\n",
        "        print(\"Failed to retrieve job attempts.\")\n",
        "\n",
        "# Set the ResourceManager host and port\n",
        "resourcemanager_host = 'localhost'\n",
        "resourcemanager_port = 8088\n",
        "\n",
        "# Set the Hadoop job details\n",
        "job_name = 'MyJob'\n",
        "jar_path = '/path/to/job.jar'\n",
        "main_class = 'com.example.MyJobClass'\n",
        "input_path = '/path/to/input'\n",
        "output_path = '/path/to/output'\n",
        "\n",
        "# Submit the Hadoop job\n",
        "app_id = submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_name, jar_path, main_class, input_path, output_path)\n",
        "\n",
        "# Monitor the job progress\n",
        "if app_id:\n",
        "    monitor_job_progress(resourcemanager_host, resourcemanager_port, app_id)\n",
        "\n",
        "    # Retrieve the job output\n",
        "    retrieve_job_output(resourcemanager_host, resourcemanager_port, app_id, output_path)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "x-MKP7a7F2rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
      ],
      "metadata": {
        "id": "AVo4Rk5wF-_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_name, jar_path, main_class, input_path, output_path, memory_mb, vcores):\n",
        "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    # Prepare the job submission payload\n",
        "    payload = {\n",
        "        \"application\": {\n",
        "            \"application-name\": job_name,\n",
        "            \"am-container-spec\": {\n",
        "                \"commands\": {\n",
        "                    \"command\": f\"yarn jar {jar_path} {main_class} {input_path} {output_path}\"\n",
        "                },\n",
        "                \"resource\": {\n",
        "                    \"memory\": memory_mb,\n",
        "                    \"vCores\": vcores\n",
        "                }\n",
        "            },\n",
        "            \"max-attempts\": 1,\n",
        "            \"resource\": {\n",
        "                \"memory\": memory_mb,\n",
        "                \"vCores\": vcores\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Submit the job\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    if response.status_code == 202:\n",
        "        data = response.json()\n",
        "        app_id = data['app']['id']\n",
        "        print(f\"Job submitted successfully. Application ID: {app_id}\")\n",
        "        return app_id\n",
        "    else:\n",
        "        print(\"Failed to submit the job.\")\n",
        "        return None\n",
        "\n",
        "def track_resource_usage(resourcemanager_host, resourcemanager_port, app_id):\n",
        "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{app_id}/appattempts\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            app_status = data['appAttempts']['appAttempt'][0]['appAttemptState']\n",
        "            resources = data['appAttempts']['appAttempt'][0]['resources']\n",
        "            allocated_memory_mb = resources['allocatedMB']\n",
        "            allocated_vcores = resources['allocatedVirtualCores']\n",
        "            print(f\"Job status: {app_status}\")\n",
        "            print(f\"Allocated memory: {allocated_memory_mb} MB\")\n",
        "            print(f\"Allocated vCores: {allocated_vcores}\")\n",
        "            print(\"-\" * 30)\n",
        "            if app_status == \"FINISHED\" or app_status == \"FAILED\" or app_status == \"KILLED\":\n",
        "                break\n",
        "        else:\n",
        "            print(\"Failed to get job status.\")\n",
        "            break\n",
        "\n",
        "        time.sleep(5)\n",
        "\n",
        "# Set the ResourceManager host and port\n",
        "resourcemanager_host = 'localhost'\n",
        "resourcemanager_port = 8088\n",
        "\n",
        "# Set the Hadoop job details\n",
        "job_name = 'MyJob'\n",
        "jar_path = '/path/to/job.jar'\n",
        "main_class = 'com.example.MyJobClass'\n",
        "input_path = '/path/to/input'\n",
        "output_path = '/path/to/output'\n",
        "\n",
        "# Set the resource requirements\n",
        "memory_mb = 1024\n",
        "vcores = 1\n",
        "\n",
        "# Submit the Hadoop job\n",
        "app_id = submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_name, jar_path, main_class, input_path, output_path, memory_mb, vcores)\n",
        "\n",
        "# Track resource usage during job execution\n",
        "if app_id:\n",
        "    track_resource_usage(resourcemanager_host, resourcemanager_port, app_id)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Hr_gpyj4GIJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
      ],
      "metadata": {
        "id": "y4nSQzpeGUjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from mrjob.job import MRJob\n",
        "import time\n",
        "\n",
        "class MyMapReduceJob(MRJob):\n",
        "\n",
        "    def configure_args(self):\n",
        "        super(MyMapReduceJob, self).configure_args()\n",
        "        self.add_passthru_arg('--split-size', type=int, default=64, help='Input split size in MB')\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        # Mapper logic goes here\n",
        "        # Replace with your actual mapper implementation\n",
        "        yield line.strip(), 1\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        # Reducer logic goes here\n",
        "        # Replace with your actual reducer implementation\n",
        "        yield key, sum(values)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set the input data file path\n",
        "    input_data = '/path/to/input/data.txt'\n",
        "\n",
        "    # Set the different split sizes to test\n",
        "    split_sizes = [64, 128, 256, 512]\n",
        "\n",
        "    for split_size in split_sizes:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the MapReduce job with the current split size\n",
        "        job_args = [input_data, '--split-size', str(split_size)]\n",
        "        job = MyMapReduceJob(args=job_args)\n",
        "        with job.make_runner() as runner:\n",
        "            runner.run()\n",
        "\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "\n",
        "        print(f\"Split Size: {split_size} MB\")\n",
        "        print(f\"Execution Time: {execution_time} seconds\")\n",
        "        print(\"-\" * 30)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DlDBVYYVGf62"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1etMgNfCEYP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UTvTxqfTDJXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}